---
layout: post
title:  "Using Machine Learning Models to Regress a Multivariate Math Function"
date:   2019-08-11 00:00:00 -0700
categories: 
---
My friend Peter is an applied mathematician who did some work in fluid dynamics. He wrote MatLab code to implement a mathematical function that calculates a maximal value of a variable K given four variables as inputs. 

As an exercise I decided to try implementing that function using some kind of machine learning (ML) model. I had no other reason for doing so â€” the function is readily implemented in fewer than two hundred lines of MatLab code that runs fast. This was purely an exploratory exercise as part of my studies in machine learning.

For my purposes it was not necessary to understand the physics of what the inputs and the outputs mean, although it should be noted that the math is somewhat complex in that it applies polynomials in various terms of higher powers.

I just treated the MatLab code as a black box that accepts certain inputs and generates corresponding outputs. My goal was to create an ML model that acts as an equivalent black box.

Peter provided a set of 549 inputs with their outputs as a representative sampling of what is generated by the MatLab code. A few of those samples are shown in the table below. **K** is the output and the other four columns are the inputs.

 <table style="width:80%; background-color:white">
	<tr><th>output: K</th><th>theta</th><th>n</th><th>m</th><th>h_star</th></tr>

	<tr><td>1.509</td><td>0.52360</td><td>3</td><td>2</td><td>0.01</td></tr>
	<tr><td>1.558</td><td>0.54105</td><td>3</td><td>2</td><td>0.01</td></tr>
	<tr><td>1.656</td><td>0.57596</td><td>3</td><td>2</td><td>0.01</td></tr>

	<tr><td>1.383</td><td>0.71558</td><td>4</td><td>3</td><td>0.01</td></tr>
	<tr><td>1.415</td><td>0.73304</td><td>4</td><td>3</td><td>0.01</td></tr>
	<tr><td>1.511</td><td>0.78540</td><td>4</td><td>3</td><td>0.01</td></tr>

	<tr><td>0.881</td><td>1.3788</td><td>9</td><td>4</td><td>0.01</td></tr>
	<tr><td>0.891</td><td>1.3963</td><td>9</td><td>4</td><td>0.01</td></tr>
	<tr><td>0.909</td><td>1.4312</td><td>9</td><td>4</td><td>0.01</td></tr>

	<tr><td>0.870</td><td>0.90757</td><td>3</td><td>2</td><td>0.001</td></tr>
	<tr><td>0.886</td><td>0.92502</td><td>3</td><td>2</td><td>0.001</td></tr>
	<tr><td>0.917</td><td>0.95993</td><td>3</td><td>2</td><td>0.001</td></tr>

	<tr><td>0.001</td><td>1.309</td><td>9</td><td>4</td><td>0.0001</td></tr>
	<tr><td>0.001</td><td>1.3265</td><td>9</td><td>4</td><td>0.0001</td></tr>
	<tr><td>0.001</td><td>1.3614</td><td>9</td><td>4</td><td>0.0001</td></tr>

	<tr><th>output: K</th><th>theta</th><th>n</th><th>m</th><th>h_star</th></tr>
 </table>

**theta** ranges continuously from 0.5326 to 1.5708. (it represents angles from 30 degrees to 90 degrees in radians.)

**n** and **m** have three possible discrete pairs of values:  (3, 2), (4, 3) and (9, 4).

**h_star** has three possible discrete values:  0.01, 0.001 and 0.0001.

The output **K** is a real number ranging continuously from 0 to about 4.1. So this can be thought of as a multi-variate regression problem with a continuous real number output.

Of the 549 examples in the data set, I used 538 for training and 11 for testing. Initially, the data set was kept in its original order but the ordering was later randomized.

The 11 test examples were chosen in a more or less stratified way in order to represent the discrete and continuous possibilities for the inputs and for the output K.

<hr width="80%" /><br />

#### **Using neural net models** ####

Almost all of my studies in ML have been in neural nets, so that was where I started. I used the Keras framework with a sequential model that makes it very easy to define the layers and that automatically infers the data flows between the layers.

My first model simplistically had a single hidden layer of 50 densely-connected units with a densely-connected output layer of 1 unit. The loss function was mean squared error, the optimizer was Adam, and the number of epochs 150.

After running the trained model on the test data, the mean absolute value of the errors was about 0.20. This was about 33% off from the correct output values. Not close at all.

